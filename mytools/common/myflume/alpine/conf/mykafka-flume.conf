producer.sources = s
producer.channels = c
producer.sinks = r

producer.sources.s.channels = c
producer.sources.s.type= netcat
producer.sources.s.bind= 0.0.0.0
producer.sources.s.port= 44444
#syslog 丢数据,rsyslog 不丢数据
#producer.sources.s.type = syslogtcp
#producer.sources.s.port = 5140
#producer.sources.s.host = 0.0.0.0
#producer.sources.s.channels = c


##老版本插件支持
#producer.sinks.r.type = org.apache.flume.plugins.KafkaSink
##kafka interface
##docker link names
#producer.sinks.r.metadata.broker.list=kafkproducer:9092
#producer.sinks.r.serializer.class=kafka.serializer.StringEncoder
#producer.sinks.r.request.required.acks=1
#producer.sinks.r.max.message.size=1000000
#producer.sinks.r.custom.topic.name=mykafka
#producer.sinks.r.channel = c

#新版本1.6原生支持
producer.sinks.r.type = org.apache.flume.sink.kafka.KafkaSink
producer.sinks.r.topic = mykafka
producer.sinks.r.brokerList = kafka1:9092,kafka2:9092,kafka3:9092
producer.sinks.r.requiredAcks = 1
producer.sinks.r.batchSize = 20
producer.sinks.r.channel = c

#producer.sinks.r.type = logger
#producer.sinks.r.channel = c

#测试ok ,为方便测试,展示更换为memory
#echo "hello,jamesmo" | nc 192.168.59.103 5140
#producer.channels.c.type   = org.apache.flume.channel.kafka.KafkaChannel
#producer.channels.c.capacity = 10000
#producer.channels.c.transactionCapacity = 1000
#producer.channels.c.brokerList=kafka1:9092,kafka2:9092,kafka3:9092
#producer.channels.c.topic=channel1
#producer.channels.c.zookeeperConnect=zk:2181


producer.channels.c.type = memory
producer.channels.c.capacity = 1000

#kafka-topics.sh --create --zookeeper 192.168.59.103:2181 --replication-factor 1 --partitions 1 --topic mykafka
#bin/flume-ng agent --conf conf --conf-file conf/flume.conf.mykafka --name producer -Dflume.root.logger=INFO,console
